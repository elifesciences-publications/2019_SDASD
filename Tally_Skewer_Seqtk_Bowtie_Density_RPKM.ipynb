{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import pickle\n",
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "### requirement\n",
    "# tally\n",
    "# skewer\n",
    "# seqtk\n",
    "# bowtie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fastq_path = '^fastq/'\n",
    "filepath = '^resuts/'\n",
    "files = ['ks175']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tally_option = '--nozip --with-quality' # will give unzipped file (will uncompile)\n",
    "\n",
    "# for skewer\n",
    "skewer_input_extension = '-unique'\n",
    "skewer_option = '-Q 10'\n",
    "skewer_adapter = 'NNNNNNCACTCGGGCACCAAGGA'\n",
    "\n",
    "# for seqtk trimfq\n",
    "fivelength_mate1 = '4'\n",
    "threelength_mate1 = '0'\n",
    "seqtk_input_extensition = '-unique-trimmed.fastq'\n",
    "seqtk_output_extensition = '-unique-trimmed-tk.fastq'\n",
    "\n",
    "\n",
    "# for bowtie\n",
    "inpath = fastq_path\n",
    "outpath = filepath\n",
    "bowtie_input_extension = seqtk_output_extensition\n",
    "\n",
    "# for mapping\n",
    "mapping = '_3map'\n",
    "\n",
    "# for RPKM\n",
    "gene_type = '_gene'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^tools/tally/tally -i ^fastq/ks175.fastq.gz -o ^fastq/ks175-unique.fastq --nozip --with-quality\n"
     ]
    }
   ],
   "source": [
    "# tally for UMI \"http://wwwdev.ebi.ac.uk/enright-dev/kraken/reaper/src/reaper-latest/doc/tally.html\"\n",
    "tally = '^tools/tally/tally'\n",
    "\n",
    "for f in files:\n",
    "    inputfile = fastq_path+f+'.fastq.gz'\n",
    "    outputfile = fastq_path+f+'-unique.fastq'\n",
    "    \n",
    "    command = '%s -i %s -o %s %s' % (\n",
    "        tally,\n",
    "        inputfile,\n",
    "        outputfile,\n",
    "        tally_option\n",
    "        )\n",
    "\n",
    "    print command\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-08-01 15:43:51.352227\n",
      "\n",
      "\n",
      "ks175\n",
      ".--. .-.\n",
      ": .--': :.-.\n",
      "`. `. : `'.' .--. .-..-..-. .--. .--.\n",
      "_`, :: . `.' '_.': `; `; :' '_.': ..'\n",
      "`.__.':_;:_;`.__.'`.__.__.'`.__.':_;\n",
      "skewer v0.2.2 [April 4, 2016]\n",
      "Parameters used:\n",
      "-- 3' end adapter sequence (-x):\u001b[0;33m\tNNNNNNCACTCGGGCACCAAGGA\n",
      "\u001b[0m-- maximum error ratio allowed (-r):\t0.100\n",
      "-- maximum indel error ratio allowed (-d):\t0.030\n",
      "-- mean quality threshold (-Q):\t\t10\n",
      "-- minimum read length allowed after trimming (-l):\t18\n",
      "-- file format (-f):\t\tSanger/Illumina 1.8+ FASTQ (auto detected)\n",
      "-- minimum overlap length for adapter detection (-k):\t3\n",
      "Thu Aug  1 15:43:51 2019\u001b[0;32m >> started\u001b[0m\n",
      "\n",
      "Thu Aug  1 15:45:07 2019\u001b[0;32m >> done\u001b[0m (75.537s)\n",
      "23714432 reads processed; of these:\n",
      "   27493 ( 0.12%) reads filtered out by quality control\n",
      "  398874 ( 1.68%) short reads filtered out after trimming by size control\n",
      "    4035 ( 0.02%) empty reads filtered out after trimming by size control\n",
      "23284030 (98.19%) reads available; of these:\n",
      "23284030 (100.00%) trimmed reads available after processing\n",
      "log has been saved to \"^fastq/ks175-unique-trimmed.log\".\n"
     ]
    }
   ],
   "source": [
    "# skewer\n",
    "skewer = '^tools/skewer022/skewer-0.2.2-linux-x86_64'\n",
    "\n",
    "log_file = filepath+'log_skewer.txt'\n",
    "###\n",
    "present = str(datetime.now())\n",
    "if not os.path.isfile(log_file):\n",
    "    logs = open(log_file, 'w')\n",
    "    logs.write(present+'\\n')\n",
    "    logs.close()\n",
    "else:\n",
    "    logs = open(log_file, 'a')\n",
    "    logs.write('\\n'+present+'\\n')\n",
    "    logs.close()     \n",
    "###\n",
    "\n",
    "for f in files:\n",
    "    file_path = fastq_path+f+skewer_input_extension+'.fastq'\n",
    "    \n",
    "    command = '%s %s -x %s %s >> %s' % (\n",
    "        skewer,\n",
    "        skewer_option,\n",
    "        skewer_adapter,\n",
    "        file_path,\n",
    "        log_file\n",
    "        ) \n",
    "    \n",
    "    logs = open(log_file, 'a+')\n",
    "    logs.write('\\n\\n'+f+'\\n')\n",
    "    logs.close()\n",
    "    \n",
    "    os.system(command)\n",
    "    \n",
    "print \n",
    "for line in open(log_file, 'r'):\n",
    "    print line[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seqtk trimfq -b 4 -e 0 ^fastq/ks175-unique-trimmed.fastq > ^fastq/ks175-unique-trimmed-tk.fastq\n"
     ]
    }
   ],
   "source": [
    "for f in files:\n",
    "    inputfile_mate1 = fastq_path+f+seqtk_input_extensition\n",
    "    outputfile_mate1 = fastq_path+f+seqtk_output_extensition\n",
    "\n",
    "    command_mate1 = 'seqtk trimfq -b %s -e %s %s > %s'%(\n",
    "        fivelength_mate1,\n",
    "        threelength_mate1,\n",
    "        inputfile_mate1,\n",
    "        outputfile_mate1\n",
    "        )    \n",
    "\n",
    "    print command_mate1\n",
    "    os.system(command_mate1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "2019-08-01 15:45:14.308918\n",
      "\n",
      "\n",
      "ks175\n",
      "bowtie -v 2 -y -m 1 -a --best --strata -S -p 30 --un ^resuts/alignments/ladder/ks175_nomatch.fastq --max ^resuts/alignments/ladder/ks175_multi.fastq --al ^resuts/alignments/ladder/ks175_match.fastq ^index/ladder/ladder ^fastq/ks175-unique-trimmed-tk.fastq ^resuts/tmpds/ks175_ladder_match.SAM 1>> ^resuts/log_bowtie.txt 2>> ^resuts/log_bowtie.txt\n",
      "# reads processed: 23284030\n",
      "# reads with at least one reported alignment: 194507 (0.84%)\n",
      "# reads that failed to align: 22806818 (97.95%)\n",
      "# reads with alignments suppressed due to -m: 282705 (1.21%)\n",
      "Reported 194507 alignments to 1 output stream(s)\n",
      "\n",
      "bowtie -v 2 -y -m 1 -a --best --strata -S -p 30 --un ^resuts/alignments/tRNA/ks175_nomatch.fastq --max ^resuts/alignments/tRNA/ks175_multi.fastq --al ^resuts/alignments/tRNA/ks175_match.fastq ^index/MG1655/Ec_tRNA ^resuts/alignments/ladder/ks175_nomatch.fastq ^resuts/tmpds/ks175_tRNA_match.SAM 1>> ^resuts/log_bowtie.txt 2>> ^resuts/log_bowtie.txt\n",
      "# reads processed: 22806818\n",
      "# reads with at least one reported alignment: 919018 (4.03%)\n",
      "# reads that failed to align: 19814594 (86.88%)\n",
      "# reads with alignments suppressed due to -m: 2073206 (9.09%)\n",
      "Reported 919018 alignments to 1 output stream(s)\n",
      "\n",
      "bowtie -v 2 -y -m 1 -a --best --strata -S -p 30 --un ^resuts/alignments/rRNA/ks175_nomatch.fastq --max ^resuts/alignments/rRNA/ks175_multi.fastq --al ^resuts/alignments/rRNA/ks175_match.fastq ^index/MG1655/rRNA_MG1655 ^resuts/alignments/tRNA/ks175_nomatch.fastq ^resuts/tmpds/ks175_rRNA_match.SAM 1>> ^resuts/log_bowtie.txt 2>> ^resuts/log_bowtie.txt\n",
      "# reads processed: 19814594\n",
      "# reads with at least one reported alignment: 533178 (2.69%)\n",
      "# reads that failed to align: 7314942 (36.92%)\n",
      "# reads with alignments suppressed due to -m: 11966474 (60.39%)\n",
      "Reported 533178 alignments to 1 output stream(s)\n",
      "\n",
      "bowtie -v 2 -y -m 1 -a --best --strata -S -p 30 --un ^resuts/alignments/ssrAssrS/ks175_nomatch.fastq --max ^resuts/alignments/ssrAssrS/ks175_multi.fastq --al ^resuts/alignments/ssrAssrS/ks175_match.fastq ^index/ssrAssrS/Ec_ssrA_ssrS ^resuts/alignments/rRNA/ks175_nomatch.fastq ^resuts/tmpds/ks175_ssrAssrS_match.SAM 1>> ^resuts/log_bowtie.txt 2>> ^resuts/log_bowtie.txt\n",
      "# reads processed: 7314942\n",
      "# reads with at least one reported alignment: 462121 (6.32%)\n",
      "# reads that failed to align: 6852820 (93.68%)\n",
      "# reads with alignments suppressed due to -m: 1 (0.00%)\n",
      "Reported 462121 alignments to 1 output stream(s)\n",
      "\n",
      "bowtie -v 2 -y -m 1 -a --best --strata -S -p 30 --un ^resuts/alignments/lacI_ffs/ks175_nomatch.fastq --max ^resuts/alignments/lacI_ffs/ks175_multi.fastq --al ^resuts/alignments/lacI_ffs/ks175_match.fastq ^index/lacI_ffs/Ec_lacI_ffs ^resuts/alignments/ssrAssrS/ks175_nomatch.fastq ^resuts/tmpds/ks175_lacIfss_match.SAM 1>> ^resuts/log_bowtie.txt 2>> ^resuts/log_bowtie.txt\n",
      "# reads processed: 6852820\n",
      "# reads with at least one reported alignment: 165334 (2.41%)\n",
      "# reads that failed to align: 6687484 (97.59%)\n",
      "# reads with alignments suppressed due to -m: 2 (0.00%)\n",
      "Reported 165334 alignments to 1 output stream(s)\n",
      "\n",
      "bowtie -v 2 -y -m 1 -a --best --strata -S -p 30 --un ^resuts/alignments/chr/ks175_nomatch.fastq --max ^resuts/alignments/chr/ks175_multi.fastq --al ^resuts/alignments/chr/ks175_match.fastq ^index/MG1655/e_coli_MG1655 ^resuts/alignments/lacI_ffs/ks175_nomatch.fastq ^resuts/alignments/chr/ks175_match.SAM 1>> ^resuts/log_bowtie.txt 2>> ^resuts/log_bowtie.txt\n",
      "# reads processed: 6687484\n",
      "# reads with at least one reported alignment: 5912051 (88.40%)\n",
      "# reads that failed to align: 642861 (9.61%)\n",
      "# reads with alignments suppressed due to -m: 132572 (1.98%)\n",
      "Reported 5912051 alignments to 1 output stream(s)\n",
      "\n",
      "2019-08-01 15:50:28.463202\n"
     ]
    }
   ],
   "source": [
    "log_file = outpath+'log_bowtie.txt'\n",
    "###\n",
    "present = str(datetime.now())\n",
    "if not os.path.isfile(log_file):\n",
    "    logs = open(log_file, 'w')\n",
    "    logs.write(present+'\\n')\n",
    "    logs.close()\n",
    "else:\n",
    "    logs = open(log_file, 'a')\n",
    "    logs.write('\\n'+present+'\\n')\n",
    "    logs.close()    \n",
    "###\n",
    "\n",
    "\n",
    "path_l = outpath + 'alignments/ladder/'\n",
    "path_t = outpath + 'alignments/tRNA/'\n",
    "path_r = outpath + 'alignments/rRNA/'\n",
    "path_s = outpath + 'alignments/ssrAssrS/'\n",
    "path_lf = outpath + 'alignments/lacI_ffs/'\n",
    "path_chr = outpath + 'alignments/chr/'\n",
    "path_temp = outpath + 'tmpds/'\n",
    "\n",
    "if not os.path.exists(path_l):\n",
    "    os.makedirs(path_l)\n",
    "if not os.path.exists(path_t):\n",
    "    os.makedirs(path_t)\n",
    "if not os.path.exists(path_r):\n",
    "    os.makedirs(path_r)\n",
    "if not os.path.exists(path_s):\n",
    "    os.makedirs(path_s)\n",
    "if not os.path.exists(path_lf):\n",
    "    os.makedirs(path_lf)   \n",
    "if not os.path.exists(path_chr):\n",
    "    os.makedirs(path_chr)\n",
    "if not os.path.exists(path_temp):\n",
    "    os.makedirs(path_temp)\n",
    "    \n",
    "        \n",
    "for fname in files:  \n",
    "    bowtie_command = 'bowtie -v 2 -y -m 1 -a --best --strata -S -p 30 '\\\n",
    "        '--un %s%s --max %s%s --al %s%s %s %s%s %s%s 1>> %s 2>> %s'\n",
    "        \n",
    "        # first, align to ladder index to subtract any ladder oligos that were cloned        \n",
    "    bowtie_ladder = bowtie_command %(\n",
    "        path_l,\n",
    "        fname+'_nomatch.fastq',\n",
    "        path_l,\n",
    "        fname+'_multi.fastq',\n",
    "        path_l,\n",
    "        fname+'_match.fastq',\n",
    "        '^index/ladder/ladder',\n",
    "        inpath,\n",
    "        fname + bowtie_input_extension,\n",
    "        path_temp,\n",
    "        fname + '_ladder_match.SAM',\n",
    "        log_file,\n",
    "        log_file\n",
    "        )\n",
    "    ###\n",
    "    logs = open(log_file, 'a+')\n",
    "    logs.write('\\n\\n'+fname+'\\n'+bowtie_ladder+'\\n')\n",
    "    logs.close()\n",
    "    ###\n",
    "    os.system(bowtie_ladder)        \n",
    "        \n",
    "        \n",
    "        # second, align to tRNA index\n",
    "    bowtie_tRNA = bowtie_command %(\n",
    "        path_t,\n",
    "        fname+'_nomatch.fastq',\n",
    "        path_t,\n",
    "        fname+'_multi.fastq',\n",
    "        path_t,\n",
    "        fname+'_match.fastq',\n",
    "        '^index/MG1655/Ec_tRNA',\n",
    "        path_l,\n",
    "        fname +'_nomatch.fastq',\n",
    "        path_temp,\n",
    "        fname + '_tRNA_match.SAM',\n",
    "        log_file,\n",
    "        log_file\n",
    "        )\n",
    "    ###\n",
    "    logs = open(log_file, 'a+')\n",
    "    logs.write('\\n'+bowtie_tRNA+'\\n')\n",
    "    logs.close()\n",
    "    ###\n",
    "    os.system(bowtie_tRNA)        \n",
    "        \n",
    "        \n",
    "        # third, align to the rRNA index\n",
    "    bowtie_rRNA = bowtie_command %(\n",
    "        path_r,\n",
    "        fname+'_nomatch.fastq',\n",
    "        path_r,\n",
    "        fname+'_multi.fastq',\n",
    "        path_r,\n",
    "        fname+'_match.fastq',\n",
    "        '^index/MG1655/rRNA_MG1655',\n",
    "        path_t,\n",
    "        fname +'_nomatch.fastq',\n",
    "        path_temp,\n",
    "        fname + '_rRNA_match.SAM',\n",
    "        log_file,\n",
    "        log_file\n",
    "        )\n",
    "    ###\n",
    "    logs = open(log_file, 'a+')\n",
    "    logs.write('\\n'+bowtie_rRNA+'\\n')\n",
    "    logs.close()\n",
    "    ###\n",
    "    os.system(bowtie_rRNA)         \n",
    "        \n",
    "        \n",
    "        # forth, align to the ssrA_ssrS index\n",
    "    bowtie_ssrAssrS = bowtie_command %(\n",
    "        path_s,\n",
    "        fname+'_nomatch.fastq',\n",
    "        path_s,\n",
    "        fname+'_multi.fastq',\n",
    "        path_s,\n",
    "        fname+'_match.fastq',\n",
    "        '^index/ssrAssrS/Ec_ssrA_ssrS',\n",
    "        path_r,\n",
    "        fname +'_nomatch.fastq',\n",
    "        path_temp,\n",
    "        fname + '_ssrAssrS_match.SAM',\n",
    "        log_file,\n",
    "        log_file\n",
    "        )\n",
    "    ###\n",
    "    logs = open(log_file, 'a+')\n",
    "    logs.write('\\n'+bowtie_ssrAssrS+'\\n')\n",
    "    logs.close()\n",
    "    ###\n",
    "    os.system(bowtie_ssrAssrS)\n",
    "       \n",
    "        \n",
    "        # fifth, align to the lacI_ffs index\n",
    "    bowtie_ssrAssrS = bowtie_command %(\n",
    "        path_lf,\n",
    "        fname+'_nomatch.fastq',\n",
    "        path_lf,\n",
    "        fname+'_multi.fastq',\n",
    "        path_lf,\n",
    "        fname+'_match.fastq',\n",
    "        '^index/lacI_ffs/Ec_lacI_ffs',\n",
    "        path_s,\n",
    "        fname +'_nomatch.fastq',\n",
    "        path_temp,\n",
    "        fname + '_lacIfss_match.SAM',\n",
    "        log_file,\n",
    "        log_file\n",
    "        )\n",
    "    ###\n",
    "    logs = open(log_file, 'a+')\n",
    "    logs.write('\\n'+bowtie_ssrAssrS+'\\n')\n",
    "    logs.close()\n",
    "    ###\n",
    "    os.system(bowtie_ssrAssrS)        \n",
    "\n",
    "    \n",
    "        # finally align to the chr index\n",
    "    bowtie_chr = bowtie_command %(\n",
    "        path_chr,\n",
    "        fname+'_nomatch.fastq',\n",
    "        path_chr,\n",
    "        fname+'_multi.fastq',\n",
    "        path_chr,\n",
    "        fname+'_match.fastq',\n",
    "        '^index/MG1655/e_coli_MG1655',\n",
    "        path_lf,\n",
    "        fname +'_nomatch.fastq',\n",
    "        path_chr,\n",
    "        fname + '_match.SAM',\n",
    "        log_file,\n",
    "        log_file\n",
    "        )\n",
    "    ###\n",
    "    logs = open(log_file, 'a+')\n",
    "    logs.write('\\n'+bowtie_chr+'\\n')\n",
    "    logs.close()\n",
    "    ###\n",
    "    os.system(bowtie_chr)\n",
    "    \n",
    "logs = open(log_file, 'a+')\n",
    "present = str(datetime.now())\n",
    "logs.write('\\n'+present+'\\n')\n",
    "logs.close()\n",
    "    \n",
    "print \n",
    "print \n",
    "for line in open(log_file, 'r'):\n",
    "    print line[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def three_end_mapping(samgen,GFFgen_filename):\n",
    "    from BCBio import GFF\n",
    "    from Bio import Seq\n",
    "    outputdata1={}\n",
    "    outputdata2={}\n",
    "    GFFgen=GFF.parse(GFFgen_filename)\n",
    "    for sequence in GFFgen:\n",
    "        outputdata1[sequence.id]=[0 for x in range(len(sequence))]\n",
    "        outputdata2[sequence.id]=[0 for x in range(len(sequence))]\n",
    "        for read in samgen:\n",
    "            if read[0][0] == '@':   # Ignore header lines.\n",
    "                continue\n",
    "            if read[1] == '4':      # A bowtie mismatch. These are the same as those previously saved in the files bowtie output as reads without matches and reads exceeding the -m limit. No need to write as fastq.\n",
    "                continue            \n",
    "            chrom = read[2]             # chromosome identified for read in bowtie\n",
    "            readid = read[0]            # read id\n",
    "            startp = int(read[3]) - 1    # start position. Need to subtract 1 since genomic sequence starts at 1, but list structures for read summing start at 0.\n",
    "            seq = Seq.Seq(read[9])      # sequence of the read\n",
    "            length = len(seq)           # length of read\n",
    "            if (read[1] == '16'):\n",
    "                start = startp\n",
    "                outputdata2[chrom][start] += 1\n",
    "            if (read[1] == '0'):\n",
    "                start = startp + length - 1\n",
    "                outputdata1[chrom][start] += 1\n",
    "    return [outputdata1,outputdata2]  \n",
    "\n",
    "def readcounts_to_rpm(readcounts):\n",
    "    for chrom in readcounts.keys():\n",
    "        totalread = sum(readcounts[chrom])\n",
    "        for position in range(len(readcounts[chrom])):\n",
    "            readcounts[chrom][position]/=float(totalread)\n",
    "            readcounts[chrom][position]*=1E6\n",
    "\n",
    "# makes density file \n",
    "# the last letter \"f\" in writecountsf means float\n",
    "def writecountsf(rpm,filepath):  #Resultlist it is the list of counts, filestring is the file prefix for each chr to be written.\n",
    "    import struct\n",
    "    f2=open(filepath+\"keys\",\"w\")\n",
    "    for chrom in rpm.keys():\n",
    "        f=open(filepath+\"genome\",\"wb\")\n",
    "        # used to be (filepath+chrom,\"wb\") but server fail to show a file with a name containing \"|\"\n",
    "        # rename \"genome\" to real genome name manually \n",
    "        for position in rpm[chrom]:\n",
    "            f.write(struct.pack(\"f\",position))\n",
    "            # struct.pack to make binary based on the format indicated \n",
    "            # here, the indication of format is \"f\", which means float\n",
    "        f.close()\n",
    "        f2.write(chrom+\"\\n\")\n",
    "    f2.close()\n",
    "\n",
    "# makes wig file\n",
    "def countstowig(rpm,filestring):\n",
    "    import random\n",
    "    f=open(filestring+\".wig\",\"w\")\n",
    "    filestring=filestring.partition(\"_\")[0][-3:]\n",
    "\n",
    "    random.seed(filestring)\n",
    "    c1=random.randint(0,255)\n",
    "    random.seed(filestring+\"xxx\")\n",
    "    c2=random.randint(0,255)\n",
    "    random.seed(filestring+\"000\")\n",
    "    c3=random.randint(0,255)\n",
    "\n",
    "    f.write(\"track name=tracklabel viewLimits=-5:5 color=\"+str(c1)+','+str(c2)+','+str(c3)+\"\\n\")\n",
    "    for chrom in rpm.keys():\n",
    "        if chrom[0:3]=='chr':\n",
    "            f.write(\"fixedStep  chrom=\"+chrom+\"  start=1  step=1\\n\")\n",
    "        else:\n",
    "            f.write(\"fixedStep  chrom=\\\"\"+chrom+\"\\\"  start=1  step=1\\n\")\t# Note the extra '' around chrom added for E coli. Should be fine for yeast. Actually it's not. So new code here to detect that.\n",
    "\n",
    "\n",
    "        for position in rpm[chrom]:\n",
    "            f.write(str(position)+\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3-end mapping for denisty\n",
    "\n",
    "######################################################\n",
    "import csv\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "\n",
    "out_path=filepath+'density/'\n",
    "if not os.path.exists(out_path):\n",
    "    os.makedirs(out_path)\n",
    "\n",
    "for f in files:\n",
    "    samgenmain_filename=filepath+'alignments/chr/'+f+'_match.SAM'\n",
    "    f_samgenmain=open(samgenmain_filename)\n",
    "    samgenmain=csv.reader(f_samgenmain,delimiter='\\t')\n",
    "\n",
    "    gff = 'coli.gff'\n",
    "\n",
    "    three_mapped=three_end_mapping(samgenmain,gff)\n",
    "    pickle_readcounts = out_path+f+mapping+'_readcounts.pickle'\n",
    "    with open(pickle_readcounts, 'wb') as readcounts_f:\n",
    "        pickle.dump(three_mapped,readcounts_f)\n",
    "\n",
    "    three_mapped_rpm = deepcopy(three_mapped)\n",
    "    readcounts_to_rpm(three_mapped_rpm[0])\n",
    "    readcounts_to_rpm(three_mapped_rpm[1])\n",
    "    pickle_rpm = out_path+f+mapping+'_rpm.pickle'\n",
    "    with open(pickle_rpm, 'wb') as rpm_f:\n",
    "        pickle.dump(three_mapped_rpm,rpm_f)\n",
    "        \n",
    "    density_path = out_path+f+mapping\n",
    "    writecountsf(three_mapped_rpm[0],density_path+\"_plus_\")\n",
    "    countstowig(three_mapped_rpm[0],density_path+\"_plus\")\n",
    "    writecountsf(three_mapped_rpm[1],density_path+\"_minus_\")\n",
    "    countstowig(three_mapped_rpm[1],density_path+\"_minus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RPKM\n",
    "\n",
    "in_path = filepath+'density/'\n",
    "out_path = filepath+'expression/'\n",
    "if not os.path.exists(out_path):\n",
    "    os.makedirs(out_path)\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "with open('Ecoli_Gene_TU.pickle','rb') as GT_f:\n",
    "    Gene_TU_dict = pickle.load(GT_f)\n",
    "\n",
    "for f in files:\n",
    "    expression_dict={}\n",
    "    out_pickle=out_path+f+mapping+gene_type+'_expression.pickle'\n",
    "    out_csv=out_path+f+mapping+gene_type+'_expression.csv'\n",
    "    readcounts_file = in_path+f+mapping+'_readcounts.pickle'\n",
    "    \n",
    "    with open(readcounts_file, 'rb') as rf:\n",
    "        readcounts_dict = pickle.load(rf)\n",
    "    plus_readcounts = readcounts_dict[0][readcounts_dict[0].keys()[0]]\n",
    "    minus_readcounts = readcounts_dict[1][readcounts_dict[1].keys()[0]]\n",
    "    total_read = sum(plus_readcounts)+sum(minus_readcounts)\n",
    "    \n",
    "    for key in Gene_TU_dict.keys():\n",
    "        if key in expression_dict.keys():\n",
    "            print key +' is overlapping!!!'\n",
    "            continue\n",
    "        start = int(Gene_TU_dict[key][2])-1\n",
    "        end = int(Gene_TU_dict[key][3])\n",
    "        length = end-start\n",
    "        if length < 0:\n",
    "            print key + 'is shorter than zero!!!'\n",
    "            continue\n",
    "        if Gene_TU_dict[key][0] == '+':\n",
    "            reads = float(sum(plus_readcounts[start:end]))\n",
    "        if Gene_TU_dict[key][0] == '-':\n",
    "            reads = float(sum(minus_readcounts[start:end]))\n",
    "        rpc = (reads/length)*3\n",
    "        rpkm = (((reads/total_read)*1000000)/length)*1000\n",
    "        expression_dict[key]=[]\n",
    "        expression_dict[key].append(reads)\n",
    "        expression_dict[key].append(rpc)\n",
    "        expression_dict[key].append(rpkm)\n",
    "    \n",
    "    with open(out_pickle, 'wb') as op:\n",
    "        pickle.dump(expression_dict,op)           \n",
    "    \n",
    "    rpkm_DataFrame = DataFrame.from_dict(expression_dict, orient='index')\n",
    "    rpkm_DataFrame.columns = ['reads','rpc','rpkm']\n",
    "    rpkm_DataFrame.to_csv(out_csv) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
